{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f401d20f-f217-4a8a-bf10-31f9172a960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cài đặt các thư viện cần thiết\n",
    "# !pip install pyPDF2 # Đọc PDF\n",
    "# !pip install ollama # Cài Ollama SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc65d5e-0118-4507-931c-41f6d748e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Thiết lập Ollama API để sử dụng GPT-OSS\n",
    "# Ollama cung cấp các mô hình GPT mà bạn có thể sử dụng cục bộ mà không cần phải gọi API trực tuyến.\n",
    "# Tuy nhiên, bạn cần cài đặt Ollama trên máy tính của mình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfeaaccf-58d6-4e43-b8e6-0665df15e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='gpt-oss' created_at='2025-11-13T05:20:04.3797393Z' done=True done_reason='stop' total_duration=11149020700 load_duration=198061600 prompt_eval_count=4096 prompt_eval_duration=1101904500 eval_count=667 eval_duration=9793021600 message=Message(role='assistant', content='**Điểm nổi bật của tài liệu này**  \\nTài liệu trình bày *Retrieval‑Augmented Generation (RAG)* một cách toàn diện, từ khái niệm cơ bản đến chi tiết thực tiễn. Các điểm chính mà bạn cần lưu ý là:\\n\\n| Điểm nổi bật | Nội dung chi tiết |\\n|--------------|-------------------|\\n| **Khái niệm RAG** | Hòa quyện thông tin tìm kiếm (information retrieval) với khả năng sinh văn bản (text generation) của LLM. |\\n| **Ba thách thức chính RAG giải quyết** | 1️⃣ Dữ liệu riêng tư & chuyên nghiệp  <br> 2️⃣ Thông tin mới nhất, cập nhật  <br> 3️⃣ Giảm “hallucination” (ngăn chặn sai lệch thực tế). |\\n| **Cấu trúc 4 bước** | 1️⃣ **Indexing**: Tải dữ liệu, tách (chunk), embed và lưu vào vector store. <br> 2️⃣ **Retrieval**: Lấy tài liệu liên quan dựa trên truy vấn. <br> 3️⃣ **Augmentation**: Kết hợp tài liệu lấy được vào prompt. <br> 4️⃣ **Generation**: LLM sinh câu trả lời dựa trên prompt đã tăng cường. |\\n| **So sánh với Fine‑tuning** | RAG là giải pháp chi phí thấp, dễ bảo trì, có thể mở rộng nhanh hơn nhiều so với fine‑tuning truyền thống. |\\n| **Hướng dẫn thực tiễn với LangChain** | Tài liệu còn mô tả chi tiết các thành phần cần thiết: Document Loaders, Text Splitters, Vector Stores, Retrievers, v.v. |\\n| **Các thuật ngữ quan trọng** | Cung cấp thuật ngữ như “parametric knowledge”, “embeddings”, “semantic search”, “vector store”,… giúp bạn nhanh chóng hiểu ngữ cảnh của RAG. |\\n| **Tham khảo nguồn gốc** | Trích nguồn từ CampusX và các tài nguyên học tập của Nitish Singh, đồng thời cung cấp liên kết tới video hướng dẫn. |\\n\\nTóm lại, **điểm nổi bật** của tài liệu này chính là **đưa ra một mô hình RAG toàn diện**—kết hợp lý thuyết, thực tiễn và các công cụ (LangChain) để giải quyết ba vấn đề lớn của LLM một cách hiệu quả, mở rộng và dễ bảo trì.', thinking='The user asks: \"Câu hỏi: Cái gì là điểm nổi bật trong tài liệu này?\" The document is about Retrieval Augmented Generation (RAG). The question: \"What is the highlight in this document?\" They want the standout features. Let\\'s produce answer in Vietnamese.\\n\\nKey highlight: RAG concept, advantages over fine-tuning, theoretical foundation, LangChain components, fine-tuning comparison, etc. The document is a reference guide. Should answer: The main highlight: the concept of Retrieval Augmented Generation and its pipeline, the 3 benefits, comparison with fine-tuning, theoretical foundation, etc. Provide in Vietnamese.', images=None, tool_name=None, tool_calls=None)\n",
      "Trả lời: **Điểm nổi bật của tài liệu này**  \n",
      "Tài liệu trình bày *Retrieval‑Augmented Generation (RAG)* một cách toàn diện, từ khái niệm cơ bản đến chi tiết thực tiễn. Các điểm chính mà bạn cần lưu ý là:\n",
      "\n",
      "| Điểm nổi bật | Nội dung chi tiết |\n",
      "|--------------|-------------------|\n",
      "| **Khái niệm RAG** | Hòa quyện thông tin tìm kiếm (information retrieval) với khả năng sinh văn bản (text generation) của LLM. |\n",
      "| **Ba thách thức chính RAG giải quyết** | 1️⃣ Dữ liệu riêng tư & chuyên nghiệp  <br> 2️⃣ Thông tin mới nhất, cập nhật  <br> 3️⃣ Giảm “hallucination” (ngăn chặn sai lệch thực tế). |\n",
      "| **Cấu trúc 4 bước** | 1️⃣ **Indexing**: Tải dữ liệu, tách (chunk), embed và lưu vào vector store. <br> 2️⃣ **Retrieval**: Lấy tài liệu liên quan dựa trên truy vấn. <br> 3️⃣ **Augmentation**: Kết hợp tài liệu lấy được vào prompt. <br> 4️⃣ **Generation**: LLM sinh câu trả lời dựa trên prompt đã tăng cường. |\n",
      "| **So sánh với Fine‑tuning** | RAG là giải pháp chi phí thấp, dễ bảo trì, có thể mở rộng nhanh hơn nhiều so với fine‑tuning truyền thống. |\n",
      "| **Hướng dẫn thực tiễn với LangChain** | Tài liệu còn mô tả chi tiết các thành phần cần thiết: Document Loaders, Text Splitters, Vector Stores, Retrievers, v.v. |\n",
      "| **Các thuật ngữ quan trọng** | Cung cấp thuật ngữ như “parametric knowledge”, “embeddings”, “semantic search”, “vector store”,… giúp bạn nhanh chóng hiểu ngữ cảnh của RAG. |\n",
      "| **Tham khảo nguồn gốc** | Trích nguồn từ CampusX và các tài nguyên học tập của Nitish Singh, đồng thời cung cấp liên kết tới video hướng dẫn. |\n",
      "\n",
      "Tóm lại, **điểm nổi bật** của tài liệu này chính là **đưa ra một mô hình RAG toàn diện**—kết hợp lý thuyết, thực tiễn và các công cụ (LangChain) để giải quyết ba vấn đề lớn của LLM một cách hiệu quả, mở rộng và dễ bảo trì.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import PyPDF2\n",
    "\n",
    "# Bước 1: Đọc nội dung PDF\n",
    "def extract_pdf_text(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Bước 2: Gọi GPT-OSS từ Ollama để trả lời câu hỏi\n",
    "# def ask_gpt_oss(question, context):\n",
    "#     # Tạo prompt với nội dung PDF và câu hỏi của người dùng\n",
    "#     prompt = f\"Đọc nội dung sau từ tài liệu và trả lời câu hỏi:\\n\\n{context}\\n\\nCâu hỏi: {question}\"\n",
    "    \n",
    "#     # Gọi Ollama GPT-OSS để trả lời câu hỏi\n",
    "#     response = ollama.chat(model=\"gpt-oss\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "#     return response['text']\n",
    "# def ask_gpt_oss(question, context):\n",
    "#     # Tạo prompt với nội dung PDF và câu hỏi của người dùng\n",
    "#     prompt = f\"Đọc nội dung sau từ tài liệu và trả lời câu hỏi:\\n\\n{context}\\n\\nCâu hỏi: {question}\"\n",
    "    \n",
    "#     # Gọi Ollama GPT-OSS để trả lời câu hỏi\n",
    "#     response = ollama.chat(model=\"gpt-oss\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "#     # In phản hồi để kiểm tra cấu trúc của nó\n",
    "#     print(response)  # In phản hồi đầy đủ từ Ollama để kiểm tra\n",
    "\n",
    "#     # Trả lời (giả sử phản hồi là một chuỗi hoặc có khóa khác để trích xuất)\n",
    "#     return response.get('text', 'Không có phản hồi hợp lệ từ Ollama')\n",
    "\n",
    "def ask_gpt_oss(question, context):\n",
    "    # Tạo prompt với nội dung PDF và câu hỏi của người dùng\n",
    "    prompt = f\"Đọc nội dung sau từ tài liệu và trả lời câu hỏi:\\n\\n{context}\\n\\nCâu hỏi: {question}\"\n",
    "    \n",
    "    # Gọi Ollama GPT-OSS để trả lời câu hỏi\n",
    "    response = ollama.chat(model=\"gpt-oss\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    # In ra phản hồi để kiểm tra cấu trúc\n",
    "    print(response)\n",
    "    \n",
    "    # Lấy câu trả lời từ nội dung trong message (nếu có)\n",
    "    return response.get('message', {}).get('content', 'Không có phản hồi hợp lệ từ Ollama')\n",
    "\n",
    "\n",
    "# Bước 3: Thực hiện ví dụ\n",
    "pdf_path = '1762685403093.pdf'  # Thay thế bằng đường dẫn thực tế của file PDF\n",
    "\n",
    "# Extract nội dung từ PDF\n",
    "pdf_text = extract_pdf_text(pdf_path)\n",
    "\n",
    "# Đặt câu hỏi cho chatbot\n",
    "question = \"Cái gì là điểm nổi bật trong tài liệu này?\"\n",
    "\n",
    "# Gọi GPT-OSS để trả lời câu hỏi\n",
    "answer = ask_gpt_oss(question, pdf_text)\n",
    "print(f\"Trả lời: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c2983-a606-4409-86c5-9c439c191c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
